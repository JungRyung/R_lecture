fa=c(150,160,170,180,190)
su=c(176,179,182,178,185)
fasu=data.frame(fa,su)
fasu
lm(su~fa,data=fasu)
data=read.csv("cars.csv")
data
out=lm(dist~speed,data=data)
summary(out)
plot(dist~speed,data=cars,col="blue")
abline(out,col="red")
out1=lm(dist~speed+0,data=data)
summary(lm(dist~speed+0,data=data))
plot(out1)
shapiro.test(sqrt(data$dist))
out3=lm(sqrt(dist)~speed+0,data=data)
summary(out3)
plot(out3)
cbind(new$speed,predict(out3,new,interval = "prediction"))
#????ȸ?ͺм?
data=read.csv("salary.csv")
head(data)
out=lm(salary~experience+score,data=data)
summary(out)
plot(out)
summary(out)
backward=step(out,direction="backward",trace=FALSE)
summary(backward)
both=step(out,direction="both",trace=FALSE)
summary(both)
##All subset method
install.packages("leaps")
library(leaps)
leaps=regsubsets(rating~.,data=attitude,nbest=5)
summary(leaps)
plot(leaps,scale='bic')
out_bic=lm(rating~complaints,data=attitude)
summary(out_bic)
plot(leaps,scale='Cp')
out_cp=lm(rating~complaints+learning,data=attitude)
summary(out_cp)
plot(leaps,scale="adjr2")
out_adjr=lm(rating~complaints+learning+advance,data=attitude)
summary(out_adjr)
setwd(".")
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
setwd("./")
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
getwd()
setwd()
setwd("./")
getwd()
/Users/ryung/Desktop/Development/R_lecture/term
setwd("/Users/ryung/Desktop/Development/R_lecture/term")
par(family = "AppleGothic")
getwd()
setwd("/Users/ryung/Desktop/Development/R_lecture/term")
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
setwd("../Rdata")
getwd()
setwd("../term")
par(family = "AppleGothic")
getwd()
setwd("/Users/ryung/Desktop/Development/R_lecture/Rdata")
par(family = "AppleGothic")
# 인공신경망 (ANN) :XOR 처리
install.packages("nnet")
library(nnet)
input <-matrix(c(0,0,1,1,0,1,0,1),ncol=2) #4행 2열로 만듬 #입력자료
input
output <- matrix(c(0,1,1,0)) #답을 먼저줌 #출력자료
output
ann <-nnet(input,output,maxit = 100, size = 2, decay = 0.001) #decay = 0.001 -> 가중치
ann # 2-2-1 => 2개 입력값 2개 입력치 1개 출력값
result <- predict(ann, input)
result
#-----------------------
df <-data.frame(
x1=c(1:6),
x2=c(6:1),
y=factor(c('n','n','n','y','y','y'))
)
df
str(df)
library(nnet)
model_net1 <- nnet(y ~ .,df,size=1) #size=1레이어 (적당한 레이어를 줘야함)
# weights:  5 값이 학습을 하면서 점점 오차값이 줄어드는 것을 알수 있다.
model_net1
summary(model_net1)
install.packages("devtools")
#ㄴㄴㅁㅇㅁㄴ
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(summary(model_net1))
#분류모형 예측
model_net1$fitted.values
predict(model_net1,df)
p<-predict(model_net1,df,type="class")
p
table(p,df$y)
setwd("./Rdata")
setwd("./Rdata")
par(family = "AppleGothic")
setwd("./temp")
setwd("/Users/ryung/Desktop/Development/R_lecture/Rdata")
par(family = "AppleGothic")
nrow(iris)
150*0.7
iris_train=iris[1:105,]
iris_test=iris[106:150,]
nrow(iris_train)
nrow(iris_train)
nrow(iris_test)
head(iris)
idx=sample(1:nrow(iris).size=nrow(iris)*0.7, replace = F)
idx=sample(1:nrow(iris),size=nrow(iris)*0.7, replace = F)
idx
iris_train = iris[idx,]
iris_test = iris[-idx,]
nrow(iris_train)
nrow(iris_test)
head(iris)
table(iris$Species)
table(iris_train$Species)
table(iris_test$Species)
install.packages("caret")
library(caret)
train.idx=createDataPartition(iris$Species,p=0.7,list=F)
table(iris_train$Species)
iris_train = iris[train.idx,]
table(iris_train)
table(iris_train$Species)
iris_test = iris[-train.dix,]
table(iris_test$Species)
iris_test = iris[-train.idx,]
table(iris_test$Species)
# 모델 선택
library(e1071)
naive.result = naiveBayes(iris_train, iris_train$Species, leplace = 1)
naic.pred=predict(naive.result,iris_test,type="class")
table(naive.pred, iris_test$Species)
naive.pred=predict(naive.result,iris_test,type="class")
table(naive.pred, iris_test$Species)
# Logistic Regression Model Method
library(nnet)
multi.result=multinom(Species~.,iris_train)
multi.pred=predict(multi.result,iris_test)
table(multi.pred,iris_test$Species)
confusionMatrix(multi.pred,iris_test$Species)
# Decision Tree Model Method
library(rpart)
rpart.result = rpart(Species~.,data=iris_train)
rpart.pred=predict(rpart.result,iris_test,type="class")
table(rpart.pred,iris_test$Species)
confusionMatrix(rpart.pred)
confusionMatrix(rpart.pred,iris_test$Species)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(rpart.result)
# ANN Model Method
library(nnet)
head(iris_train)
iris_train_scale = as.data.frame(sapply(iris_train[,-5],scale))
iris_train_scale
iris_test_scale = as.data.frame(sapply(iris_test[,-5], scale))
iris_test_scale
iris_train_scale$Species = iris_train$Species
iris_test_scale$Species = iris_test$Species
head(iris_train_scale)
nnet.result=nnet(Species~.,iiris_train_scale)
nnet.result=nnet(Species~.,iiris_train_scale, size=3)
nnet.result=nnet(Species~.,iris_train_scale, size=3)
nnet.pred = predict(nnet.result, iris_test_scale, type="class")
table(nnet.pred, iris_test$Species)
table(nnet.pred, iris_test_scale$Species)
confusionMatrix(nnet.pred,iris_test$Species)
confusionMatrix(nnet.pred,iris_test_scale$Species)
table(nnet.pred, iris_test$Species)
setwd("../term")
getwd()
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
helth_beverage=data[1:60,]
helth_beverage
juice_beverage=data[61:120,]
juice_beverage
tea_beverage=data[120:180,]
tea_beverage
data_feature = data[,2:3]
View(data)
cov(data)     # 공분산 함수
cov(data)     # 공분산 함수
data_feature = data[,2:3]
cov(data)     # 공분산 함수
cor(attitude)     # 상관계수
setwd("../term")
getwd()
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
helth_beverage=data[1:60,]
helth_beverage
juice_beverage=data[61:120,]
juice_beverage
tea_beverage=data[120:180,]
tea_beverage
data_feature = data[,2:3]
View(data)
cov(data)     # 공분산 함수
cor(attitude)     # 상관계수
setwd("../term")
getwd()
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
helth_beverage=data[1:60,]
helth_beverage
juice_beverage=data[61:120,]
juice_beverage
tea_beverage=data[120:180,]
tea_beverage
data_feature = data[,2:3]
View(data)
cov(data)     # 공분산 함수
cor(attitude)     # 상관계수
data = read.csv("sales_data_new.csv")
cov(data)     # 공분산 함수
cor(data)     # 상관계수
cor(attitude)     # 상관계수
cov(data)     # 공분산 함수
data = data[,3:10]
cov(data)     # 공분산 함수
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature = helth_beverage[3:10]
helth_beverage_feature
helth_beverage_feature = helth_beverage[4:10]
helth_beverage_feature
cov(helth_beverage_feature)
cor(helth_beverage_feature)
cov(helth_beverage_feature)
cor(helth_beverage_feature)
g
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature
cov(helth_beverage_feature)
cor(helth_beverage_feature)
juice_beverage=data[61:120,]
juice_beverage
juice_beverage_feature = juice_beverage[4:10]
juice_beverage_feature
cov(juice_beverage_feature)
cor(juice_beverage_feature)
tea_beverage=data[120:180,]
tea_beverage
tea_beverage_feature = tea_beverage[4:10]
tea_beverage_feature
cov(tea_beverage_feature)
cor(tea_beverage_feature)
cor(helth_beverage_feature)
cor(juice_beverage_feature)
cor(tea_beverage_feature)
confusionMatrix(as.factor(nnet.pred),as.factor(iris_test$Species))
# SVM Model Method
install.packages("kernlab")
# SVM Model Method
#install.packages("kernlab")
library(package)
# SVM Model Method
#install.packages("kernlab")
library(kernlab)
svm.result = ksvm(Species~., iris_train, kernel="rbfdot")
svm.pred = predict(svm.result, iris_test, type="response")
table(svm.pred, iris_test$Species)
confusionMatrix(svm.pred, iris_test$Species)
# RandomForest Model Method
install.packages("randomForest")
# RandomForest Model Method
#install.packages("randomForest")
library(randomForest)
rf.result = randomForest(Species~., iris_train, ntree=500)
rf.pred = predict(rf.result, iris_test, type="response")
table(rf.pred, iris_test$Species)
confusionMatrix(rf.pred, iris_test$Species)
# Package안의 데이터
data(package="MASS")
Boston=as.data.frame(MASS::Boston)
names(Boston)
nrow(Boston)
idx=sample(1:nrow(Boston),size=nrow(Boston)*0.7,replace = F)
Boston_train = Boston[idx,]
Boston_test = Boston[-idx,]
dim(Boston_train) ; dim(Boston_test)
lm.fit = lm(medv~., data = Boston_train)
summary(lm.fit)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=Boston_test)
mean((lm.yhat2-Boston_test$medv)^2)
kk=mean((lm.yhat2-Boston_test$medv)^2)
sqrt(kk)
plot(lm.yhat2,Boston_test$medv)
abline(a=0,b=1,col=2)
## 다중회귀분석 기법 사용
head(Boston_train)
lm.fit = lm(QTY~., data = helth_beverage_feature)
summary(lm.fit)
## 의사결정트리 기법 사용
install.packages("tree")
## 의사결정트리 기법 사용
#install.packages("tree")
library(tree)
tree.fit=tree(medv~., data = Boston_train)
summary(tree.fit)
plot(tree.fit)
text(tree.fit,pretty=0)
tree.yhat = predict(tree.fit, newdata=Boston_test)
mean((tree.yhat-Boston_test$medv)^2)
library(rpart)
rpart.fit = rpart(medv~., data=Boston_train)
summary(rpart.fit)
library(rpart.plot)
rpart.plot(rpart.fit, digits = 3, typer=0, extra=1, fallen.leaves=F, cex=1)
rpart.plot(rpart.fit, digits = 3, type=0, extra=1, fallen.leaves=F, cex=1)
kk=mean((tree.yhat-Boston_test$medv)^2)
sqrt(kk)
rpart.yhat = predict(rpart.fit, newdata=Boston_test)
mean((rpart.yhat-Boston_test$medv)^2)
kk=mean((rpart.yhat-Boston_test$medv)^2)
sqrt(kk)
# 인공 신경망 기법 사용
## 정규화 함수 작성하기
normalize = function(x){return((x-min(x))/(max(x)-min(x)))}
Boston_train_norm = as.data.frame(sapply(Boston_train, normalize))
Boston_test_norm = as.data.frame(sapply(Boston_test, normalize))
## nnet 함수 사용한 인공 신경망 분석
library(nnet)
nnet.fit = nnet(medv~., data=Boston_train_norm, size=5)
nnet.yhat = predict(nnet.fit, newdata=Boston_test_norm, type="raw")
mean((nnet.yhat-Boston_test_norm$medv)^2)
## neuralnet 함수 사용한 인공 신경망 분석
library(neuralnet)
## neuralnet 함수 사용한 인공 신경망 분석
install.packages("neuralnet")
## neuralnet 함수 사용한 인공 신경망 분석
#install.packages("neuralnet")
library(neuralnet)
neural.fit = neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio
+black+lstat, data = Boston_train_norm, hidden=5)
neural.results = compute(neural.fit, Boston_test_norm[1:13])
neural.yhat = neural.results$net.result
mean((neural.yhat-Boston_test_norm$medv)^2)
plot(neural.fit)
plot(neural.fit)
library(randomForest)
set.seed(1)
rf.fit = randomForest(medv~., data = Boston_train, mtry = 6, importance = T)
rf.fit
plot(rf.fit)
plot(neural.fit)
plot(rf.fit)
plot(neural.fit)
importance(rf.fit)
varImpPlot(rf.fit)
rf.yhat = predict(rf.fit, newdata=Boston_test)
mean((rf.yhat-Boston_test$medv)^2)
sqrt(kk)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.fit = lm(QTY~., data = juice_beverage_feature)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=Boston_test)
kk=mean((lm.yhat2-Boston_test$medv)^2)
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withnes=c()
km.out.between=c()
km.out.withnes[i-1] = km.out$betweenss
km.out.withness=c()
km.out.between=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.withness[i-1] = km.out$betweenss
}
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.withness[i-1] = km.out$betweenss
}
km.out.withness[i-1] = km.out$betweenss
print(paste0("k= ",i))
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.withness[i-1] = km.out$betweenss
print(paste0("k= ",i))
}
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
print(paste0("k= ",i))
}
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
data.frame(kk,km.out.withness,km.out.between)
data.frame(kk,km.out.withness,km.out.between)
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
data.frame(kk,km.out.withness,km.out.between)
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
data.frame(kk,km.out.withness,km.out.between)
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
tt=data.frame(kk,km.out.withness,km.out.between)
plot(kk=c(1:7),tt$km.out.withness,type='b')
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
tt=data.frame(kk,km.out.withness,km.out.between)
plot(kk=c(1:7),tt$km.out.withness,type='b')
plot(kk=c(1:7),tt$km.out.between,type='b')
