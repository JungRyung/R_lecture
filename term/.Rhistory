text(tt,kk,paste0(kk,"대"),pos=3,col=2,cex=2)
df=as.data.frame(mpg %>%
filter(class=="suv")%>%
group_by(manufacturer)%>%
summarise(mean_cty=mean(cty),
mean_hwy=mean(hwy))%>%
arrange(desc(mean_cty))%>%
head(5))
# 08-2
mpg <- as.data.frame(ggplot2::mpg)
midwest <- as.data.frame(ggplot2::midwest)
library(ggplot2)
## Q1. x축은 cty, y축은 hwy로 된 산점도
ggplot(data = mpg, aes(x = cty, y = hwy)) + geom_point()
## Q2. x축은 poptotal, y축은 popasian으로 된 산점도
ggplot(data = midwest, aes(x = poptotal, y = popasian)) + geom_point() +
xlim(0,500000) +
ylim(0,10000)
# 08-3
## Q1.
kk=table(mpg$class)
tt=barplot(kk,col=rainbow(8),ylim=c(0,70))
text(tt,kk,paste0(kk,"대"),pos=3,col=2,cex=2)
df=as.data.frame(mpg %>%
filter(class=="suv")%>%
group_by(manufacturer)%>%
summarise(mean_cty=mean(cty),
mean_hwy=mean(hwy))%>%
arrange(desc(mean_cty))%>%
head(5))
# 08-2
par(family='Unbatang')
mpg <- as.data.frame(ggplot2::mpg)
midwest <- as.data.frame(ggplot2::midwest)
library(ggplot2)
## Q1. x축은 cty, y축은 hwy로 된 산점도
ggplot(data = mpg, aes(x = cty, y = hwy)) + geom_point()
## Q2. x축은 poptotal, y축은 popasian으로 된 산점도
ggplot(data = midwest, aes(x = poptotal, y = popasian)) + geom_point() +
xlim(0,500000) +
ylim(0,10000)
# 08-3
## Q1.
kk=table(mpg$class)
tt=barplot(kk,col=rainbow(8),ylim=c(0,70))
text(tt,kk,paste0(kk,"대"),pos=3,col=2,cex=2)
df=as.data.frame(mpg %>%
filter(class=="suv")%>%
group_by(manufacturer)%>%
summarise(mean_cty=mean(cty),
mean_hwy=mean(hwy))%>%
arrange(desc(mean_cty))%>%
head(5))
df
ggplot(data=df,aes(x=reorder(manufacturer,mean_cty),y=mean_cty)) +
geom_col(fill=rainbow(5)) +
coord_flip() +
xlab("차종") +
ylab("평균도시연비")
tt=barplot(kk,col=rainbow(8),ylim=c(0,70))
text(tt,kk,paste0(kk,"대"),pos=3,col=2,cex=2)
# 08-3
## Q1.
kk=table(mpg$class)
tt=barplot(kk,col=rainbow(8),ylim=c(0,70))
text(tt,kk,paste0(kk,"대"),pos=3,col=2,cex=2)
df=as.data.frame(mpg %>%
filter(class=="suv")%>%
group_by(manufacturer)%>%
summarise(mean_cty=mean(cty),
mean_hwy=mean(hwy))%>%
arrange(desc(mean_cty))%>%
head(5))
df
ggplot(data=df,aes(x=reorder(manufacturer,mean_cty),y=mean_cty)) +
geom_col(fill=rainbow(5)) +
coord_flip() +
xlab("차종") +
ylab("평균도시연비")
# 08-2
mpg <- as.data.frame(ggplot2::mpg)
midwest <- as.data.frame(ggplot2::midwest)
setwd("/Users/ryung/Desktop/Development/R_lecture/Rdata")
par(family = "AppleGothic")
nrow(iris)
150*0.7
# train set과 test set으로 분할 (7:3)
iris_train=iris[1:105,]
iris_test=iris[106:150,]
nrow(iris_train)
nrow(iris_test)
# 위와같이 분할하면 데이터가 일관되지 못하고 편중되기 때문에 무작위성이 필요
head(iris)
idx=sample(1:nrow(iris),size=nrow(iris)*0.7, replace = F) # 무작위 추출 인덱스 생성
idx
iris_train = iris[idx,]
iris_test = iris[-idx,]
nrow(iris_train)
nrow(iris_test)
head(iris)
# 원데이터와 분할된 데이터 세트에서 목적변수 속성들의 빈도 일관성 여부 확인
table(iris$Species)
table(iris_train$Species)
table(iris_test$Species)
# caret package를 사용해서 각 속성들의 빈도가 일관되게 만듦
#install.packages("caret")
library(caret)
train.idx=createDataPartition(iris$Species,p=0.7,list=F)
iris_train = iris[train.idx,]
table(iris_train$Species)
iris_test = iris[-train.idx,]
table(iris_test$Species)
# 모델 선택
# Naive Bayes Model Method
library(e1071)
naive.result = naiveBayes(iris_train, iris_train$Species, leplace = 1)
naive.pred=predict(naive.result,iris_test,type="class")
table(naive.pred, iris_test$Species)
confusionMatrix(naive.pred, iris_)
# Logistic Regression Model Method
library(nnet)
multi.result=multinom(Species~.,iris_train)
multi.pred=predict(multi.result,iris_test)
table(multi.pred,iris_test$Species)
confusionMatrix(multi.pred,iris_test$Species)
# Decision Tree Model Method
library(rpart)
rpart.result = rpart(Species~.,data=iris_train)
rpart.pred=predict(rpart.result,iris_test,type="class")
table(rpart.pred,iris_test$Species)
confusionMatrix(rpart.pred,iris_test$Species)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(rpart.result)
# ANN Model Method
library(nnet)
head(iris_train)
# ANN 모델을 사용하기 전 data set 정규화
iris_train_scale = as.data.frame(sapply(iris_train[,-5],scale))
iris_test_scale = as.data.frame(sapply(iris_test[,-5], scale))
iris_train_scale$Species = iris_train$Species
iris_test_scale$Species = iris_test$Species
head(iris_train_scale)
nnet.result=nnet(Species~.,iris_train_scale, size=3)
nnet.pred = predict(nnet.result, iris_test_scale, type="class")
table(nnet.pred, iris_test$Species)
confusionMatrix(as.factor(nnet.pred),as.factor(iris_test$Species))
# SVM Model Method
#install.packages("kernlab")
library(kernlab)
svm.result = ksvm(Species~., iris_train, kernel="rbfdot")
svm.pred = predict(svm.result, iris_test, type="response")
table(svm.pred, iris_test$Species)
confusionMatrix(svm.pred, iris_test$Species)
# RandomForest Model Method
#install.packages("randomForest")
library(randomForest)
rf.result = randomForest(Species~., iris_train, ntree=500)
rf.pred = predict(rf.result, iris_test, type="response")
table(rf.pred, iris_test$Species)
confusionMatrix(rf.pred, iris_test$Species)
# Package안의 데이터
data(package="MASS")
Boston=as.data.frame(MASS::Boston)
names(Boston)
nrow(Boston)
idx=sample(1:nrow(Boston),size=nrow(Boston)*0.7,replace = F)
Boston_train = Boston[idx,]
Boston_test = Boston[-idx,]
dim(Boston_train) ; dim(Boston_test)
## 다중회귀분석 기법 사용
lm.fit = lm(medv~., data = Boston_train)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=Boston_test)
kk=mean((lm.yhat2-Boston_test$medv)^2)
sqrt(kk)
plot(lm.yhat2,Boston_test$medv)
abline(a=0,b=1,col=2)
## 의사결정트리 기법 사용
#install.packages("tree")
library(tree)
tree.fit=tree(medv~., data = Boston_train)
summary(tree.fit)
plot(tree.fit)
text(tree.fit,pretty=0)
tree.yhat = predict(tree.fit, newdata=Boston_test)
kk=mean((tree.yhat-Boston_test$medv)^2)
sqrt(kk)
## rpart를 통한 의사결정트리분석 방법
library(rpart)
rpart.fit = rpart(medv~., data=Boston_train)
summary(rpart.fit)
library(rpart.plot)
rpart.plot(rpart.fit, digits = 3, type=0, extra=1, fallen.leaves=F, cex=1)
rpart.yhat = predict(rpart.fit, newdata=Boston_test)
kk=mean((rpart.yhat-Boston_test$medv)^2)
sqrt(kk)
# 인공 신경망 기법 사용
## 정규화 함수 작성하기
normalize = function(x){return((x-min(x))/(max(x)-min(x)))}
Boston_train_norm = as.data.frame(sapply(Boston_train, normalize))
Boston_test_norm = as.data.frame(sapply(Boston_test, normalize))
## nnet 함수 사용한 인공 신경망 분석
library(nnet)
nnet.fit = nnet(medv~., data=Boston_train_norm, size=5)
nnet.yhat = predict(nnet.fit, newdata=Boston_test_norm, type="raw")
mean((nnet.yhat-Boston_test_norm$medv)^2)
## neuralnet 함수 사용한 인공 신경망 분석 (시각화를 위해서 모든 feature를 직접 쳐 넣어야한다)
#install.packages("neuralnet")
library(neuralnet)
neural.fit = neuralnet(medv~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio
+black+lstat, data = Boston_train_norm, hidden=5)
neural.results = compute(neural.fit, Boston_test_norm[1:13])
neural.yhat = neural.results$net.result
mean((neural.yhat-Boston_test_norm$medv)^2)
plot(neural.fit)
library(randomForest)
set.seed(1)
rf.fit = randomForest(medv~., data = Boston_train, mtry = 6, importance = T)
rf.fit
plot(rf.fit)
importance(rf.fit)
varImpPlot(rf.fit)
rf.yhat = predict(rf.fit, newdata=Boston_test)
kk=mean((rf.yhat-Boston_test$medv)^2)
sqrt(kk)
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withness
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
tt=data.frame(kk,km.out.withness,km.out.between)
plot(kk=c(1:7),tt$km.out.withness,type='b')
plot(kk=c(1:7),tt$km.out.between,type='b')
# 자율학습모델 (Unsupervised Model)
# K-means model method
iris2 = iris[,1:4]
iris2
km.out.withness=c()
km.out.between=c()
kk=c()
for(i in 2:7){
set.seed(1)
km.out=kmeans(iris2, centers=i)
km.out.withness[i-1] = km.out$tot.withinss
km.out.between[i-1] = km.out$betweenss
kk=c(kk,(paste0("k=",i)))
}
tt=data.frame(kk,km.out.withness,km.out.between)
plot(kk=c(1:7),tt$km.out.withness,type='b')
plot(kk=c(1:7),tt$km.out.between,type='b')
km.out.k3=kmeans(iris2,centers=3)
km.out.k3$centers
km.out.k3$cluster
km.out.k3$size
table(km.out.k3$cluster, iris$Species)
plot(iris2[,1:2], col=km.out.k3$cluster, pch=ifelse(km.out.k3$cluster==1, 16, ifelse(km.out.k3$cluster==2,17,18),
cex-2) ; points(km.out.k3$centers, col=1:3, pch=16:18, cex=5))
plot(iris2[,1:2], col=km.out.k3$cluster, pch=ifelse(km.out.k3$cluster==1, 16, ifelse(km.out.k3$cluster==2,17,18)),
cex-2)
plot(iris2[,1:2], col=km.out.k3$cluster, pch=ifelse(km.out.k3$cluster==1, 16, ifelse(km.out.k3$cluster==2,17,18)),
cex-2)
plot(iris2[,1:2], col=km.out.k3$cluster, pch=ifelse(km.out.k3$cluster==1, 16, ifelse(km.out.k3$cluster==2,17,18)),
cex=2)
points(km.out.k3$centers, col=1:3, pch=16:18, cex=5)
USArrests
pc1 = princomp(USArrests,cor=T)
plot(pc1)
summary(pc1)
pc1$center
pc1$scale
pc1$loadings
pc1$scores
plot(pc1$scores[,1],pc1$scores[,2], xlab="Z1", ylab="Z2")
abline(v=0, h=0, col="gray")
biplot(pc1, cex=0.7)
abline(v=0, h=0, col="gray")
setwd("../Rdata")
getwd()
View(attitude)
cov(attitude)     # 공분산 함수
cor(attitude)     # 상관계수.
with(attitude,cor.test(rating, complaints))
cor.test(attitude$rating,attitude$complaints)
plot(attitude$rating,attitude$complaints)
fa=c(150,160,170,180,190)
su=c(176,179,182,178,185)
fasu=data.frame(fa,su)
fasu
lm(su~fa,data=fasu)
data=read.csv("cars.csv")
data
out=lm(dist~speed,data=data)
summary(out)
plot(dist~speed,data=cars,col="blue")
abline(out,col="red")
summary(out)
plot(dist~speed,data=cars,col="blue")
abline(out,col="red")
out1=lm(dist~speed+0,data=data)
summary(lm(dist~speed+0,data=data))
plot(out1)
shapiro.test(sqrt(data$dist))
shapiro.test(data$dist)
shapiro.test(log(data$dist))
shapiro.test(sqrt(data$dist))
out3=lm(sqrt(dist)~speed+0,data=data)
summary(out3)
plot(out3)
cbind(new$speed,predict(out3,new,interval = "prediction"))
#????ȸ?ͺм?
data=read.csv("salary.csv")
head(data)
out=lm(salary~experience+score,data=data)
summary(out)
plot(out)
summary(out)
backward=step(out,direction="backward",trace=FALSE)
summary(backward)
both=step(out,direction="both",trace=FALSE)
summary(both)
backward=step(out,direction="backward",trace=FALSE)
summary(backward)
both=step(out,direction="both",trace=FALSE)
summary(both)
##All subset method
install.packages("leaps")
##All subset method
#install.packages("leaps")
library(leaps)
leaps=regsubsets(rating~.,data=attitude,nbest=5)
summary(leaps)
plot(leaps,scale='bic')
cor(helth_beverage_feature)
setwd("../term")
getwd()
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
## 건강음료에 대한 다중 회귀 분석
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature = helth_beverage[4:10]
helth_beverage_feature
cov(helth_beverage_feature)
cor(helth_beverage_feature)
## 잔차도
plot(lm(QTY~SALEDAY,data=helth_beverage_feature))
## 잔차도
plot(lm(QTY~SALEDAY+0,data=helth_beverage_feature))
## 잔차도
plot(lm(QTY^2~SALEDAY+0,data=helth_beverage_feature))
## 잔차도
plot(lm(QTY^2~SALEDAY+0,data=helth_beverage_feature))
## 잔차도
plot(lm(QTY~SALEDAY+0,data=helth_beverage_feature))
## 잔차도
out=plot(lm(QTY~SALEDAY+0,data=helth_beverage_feature))
shapiro.test(resid(out))
shapiro.test(out)
plot(out)
## 잔차도
out=plot(lm(QTY~SALEDAY+0,data=helth_beverage_feature))
shapiro.test(out)
shapiro.test(resid(out))
plot(QTY~SALEDAY)
plot(QTY~SALEDAY,data=helth_beverage_feature)
cor(helth_beverage_feature)
model = lm(QTY~SALEDAY+PRICE)
model = lm(QTY~SALEDAY+PRICE,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE,helth_beverage_feature)
summary(model)
model = lm(QTY~.,helth_beverage_feature)
summary(model)
## Forward selection
model = lm(QTY~SALEDAY,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP+RAIN_DAY,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP+RAIN_DAY+HOLIDAY,helth_beverage_feature)
summary(model)
## All Subsets Regression
library(leaps)
leaps=regsubsets(QTY~.,data=helth_beverage_feature,nbest=5)
summary(leaps)
plot(leaps)
#best
model = lm(QTY~SALEDAY+PRICE+RAIN_DAY,helth_beverage_feature)
summary(model)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
## 모든 feature를 포함한 다중회귀
lm.fit = lm(QTY~., data = helth_beverage_feature)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=Boston_test)
kk=mean((lm.yhat2-Boston_test$medv)^2)
sqrt(kk)
plot(lm.yhat2,Boston_test$medv)
abline(a=0,b=1,col=2)
## 모든 feature를 포함한 다중회귀
lm.fit = lm(QTY~., data = helth_beverage_feature)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=helth_beverage_feature)
helth_beverage_feature_train = helth_beverage_feature[1:42,]
helth_beverage_feature_test = helth_beverage_feature[42:60,]
dim(helth_beverage_feature_train)
dim(helth_beverage_feature_test)
setwd("../term")
getwd()
par(family = "AppleGothic")
data = read.csv("sales_data_new.csv")
head(data)
data[,3:10]
## 건강음료에 대한 다중 회귀 분석
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature = helth_beverage[4:10]
helth_beverage_feature
cov(helth_beverage_feature)
## 건강음료에 대한 다중 회귀 분석
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature = helth_beverage[4:10]
helth_beverage_feature
cov(helth_beverage_feature)
cor(helth_beverage_feature)
cor(helth_beverage_feature)
## 과즙음료에 대한 다중 회귀 분석
juice_beverage=data[61:120,]
juice_beverage
juice_beverage_feature = juice_beverage[4:10]
juice_beverage_feature
cov(juice_beverage_feature)
cor(juice_beverage_feature)
## 차 음료에 대한 다중 회귀 분석
tea_beverage=data[120:180,]
tea_beverage
tea_beverage_feature = tea_beverage[4:10]
tea_beverage_feature
cov(tea_beverage_feature)
cor(tea_beverage_feature)
## 건강음료에 대한 다중 회귀 분석
helth_beverage=data[1:60,]
helth_beverage
helth_beverage_feature = helth_beverage[4:10]
helth_beverage_feature
cov(helth_beverage_feature)
cor(helth_beverage_feature)
## 모든 feature를 포함한 다중회귀
lm.fit = lm(QTY~., data = helth_beverage_feature)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=helth_beverage_feature)
## Forward selection
model = lm(QTY~SALEDAY,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP+RAIN_DAY,helth_beverage_feature)
summary(model)
model = lm(QTY~SALEDAY+PRICE+ITEM_CNT+MAXTEMP+RAIN_DAY+HOLIDAY,helth_beverage_feature)
## All Subsets Regression
library(leaps)
leaps=regsubsets(QTY~.,data=helth_beverage_feature,nbest=5)
summary(leaps)
plot(leaps)
## 모든 feature를 포함한 다중회귀
lm.fit = lm(QTY~., data = helth_beverage_feature)
summary(lm.fit)
## forward와 backward 방식을 전부 써서 좋은 결과만 가져온다.(both)
lm.fit2 = step(lm.fit,method="both")
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=helth_beverage_feature)
summary(lm.fit2)
lm.yhat2=predict(lm.fit2,newdata=helth_beverage_feature)
kk=mean((lm.yhat2-helth_beverage_feature$QTY)^2)
